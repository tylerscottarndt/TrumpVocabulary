{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0, 1]), array([2000, 2000])), (array([0, 1]), array([2000, 2000])))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class load_file:\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "\n",
    "    def get_data(self):\n",
    "        hal = np.load(self.file)\n",
    "        X_train, y_train, X_test, y_test,vocab_size, embedding_matrix = [hal[f] for f in hal.files]\n",
    "        return X_train, y_train, X_test, y_test,vocab_size, embedding_matrix\n",
    "\n",
    "\n",
    "df = load_file('ObamaSplitData.npz')\n",
    "X_train, y_train, X_test, y_test,vocab_size, embedding_matrix = df.get_data()\n",
    "np.unique(y_train, return_counts=True), np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense\n",
    "from keras.constraints import max_norm\n",
    "from keras.regularizers import l1,l2\n",
    "from keras import optimizers\n",
    "import keras\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "maxlen = 100\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=maxlen)\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=optimizers.adam(lr=.00001), loss='binary_crossentropy', metrics=['acc'])\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "print(model.summary())\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=300, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=1)\n",
    "print('Training Accuracy is {}'.format(accuracy*100))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test,y_test)\n",
    "\n",
    "\n",
    "print('Testing Accuracy is {} '.format(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
